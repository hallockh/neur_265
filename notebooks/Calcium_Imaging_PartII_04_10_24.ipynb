{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO++dX9/V8aKoSviI9lGvQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hallockh/neur_265/blob/main/notebooks/Calcium_Imaging_PartII_04_10_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calcium Imaging - Part Deux\n",
        "\n",
        "In our first notebook on calcium imaging, we talked about what calcium imaging is, how neuroscientists do it, and then we related traces from individual ROIs (cells) to spatial positions in the zebrafish brain. In this notebook, we will build off of what we learned to relate fluorescent signals to behavior, both in the zebrafish (2-photon imaging) and in the mouse (1-photon imaging)."
      ],
      "metadata": {
        "id": "hnAfD8RKJHRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##By the end of this notebook, you will be able to:\n",
        "\n",
        "- Evaluate relationships between fluorescent signal and stimulus input\n",
        "- Relate these fluorescent signals back to areas of the brain\n",
        "- Isolate event-related fluorescenct signals from multiple neurons in the mouse brain during fear conditioning"
      ],
      "metadata": {
        "id": "mM6Ud2u0Jknt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Introduction\n",
        "\n",
        "We will be working with two datasets today. The first dataset contains fluorescent signal from a zebrafish brain during LED (light) pulses. The second dataset contains fluorescent signal from multiple neurons imaged in the mouse anterior cingulate cortex (ACC) during fear conditioning - a paradigm in which the mouse is placed in an chamber and given brief foot-shocks that coincide with an auditory tone. For our mouse dataset, as in our previous notebook, ROIs have already been extracted - what we now have to do is evaluate the\n",
        "Î”F/F signals from each ROI. For our zebrafish dataset, we're going to start with a raw imaging movie and extract fluorescent traces."
      ],
      "metadata": {
        "id": "g5vcN3YbKOze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a buncha modules!\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib as mpl\n",
        "from matplotlib.ticker import FixedLocator\n",
        "from matplotlib.colorbar import Colorbar\n",
        "from skimage import io\n",
        "import matplotlib.ticker as mticker\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from matplotlib import gridspec"
      ],
      "metadata": {
        "id": "NlxtAjy2LAuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by importing the movie from our shared Google Drive folder:"
      ],
      "metadata": {
        "id": "7NbbB760LLrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access its folders\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "lidS0sXwLzyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeff6976-f9ea-4922-b9e1-1d0cf09b31ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the video (hint: put the path to your file in the string)\n",
        "\n",
        "LED_video = io.imread('')"
      ],
      "metadata": {
        "id": "pV68AfuPNWp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first make a helper function to normalize our data:"
      ],
      "metadata": {
        "id": "H7Q-WUTFSkza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalizeData(data):\n",
        "    if type(data).__module__ == np.__name__: # if we work with numpy data\n",
        "        normVector = (data - data.min())/(data.max() - data.min())\n",
        "        return normVector\n",
        "\n",
        "    dataCopy = copy.deepcopy(data)\n",
        "    for i in range(len(dataCopy)):\n",
        "        min_data = np.min(dataCopy.iloc[i])\n",
        "        max_data = np.max(dataCopy.iloc[i])\n",
        "        tt = (np.array(dataCopy.iloc[i]) - min_data) / (max_data - min_data)\n",
        "        dataCopy.iloc[i] = tt\n",
        "    return dataCopy"
      ],
      "metadata": {
        "id": "iHBnvwWsSxTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, a function to model the shape of the LED input (sudden onset with exponential decay):"
      ],
      "metadata": {
        "id": "ADeajiTRS1hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modelOnset(tau=200, phase = 300, n=700):\n",
        "    onset_model = np.zeros(n) # pre-allocatre memory with empty array of zeros\n",
        "    decay = lambda t: np.exp(-t/tau) # exponential decay\n",
        "    vfunc = np.vectorize(decay)\n",
        "    onset_model[phase:] = vfunc(np.array(range(phase, n))) # in our experiment LED turns on at frame 300\n",
        "\n",
        "    return onset_model"
      ],
      "metadata": {
        "id": "qEKyXD2-TBiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now display the first frame of the video file that we imported from Google Drive:"
      ],
      "metadata": {
        "id": "o548rcVTTF1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Video shape: \", LED_video.shape)\n",
        "\n",
        "LED_video_frames = LED_video.shape[0]\n",
        "LED_video_y_pixels = LED_video.shape[1]\n",
        "LED_video_x_pixels = LED_video.shape[2]\n",
        "\n",
        "video_first_frame = copy.deepcopy(LED_video[0])\n",
        "plt.imshow(video_first_frame);"
      ],
      "metadata": {
        "id": "VKgk7nS2TIj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Try creating an image of a bunch of other video frames. How do the frames change as you progress through the movie?"
      ],
      "metadata": {
        "id": "kZ3OUBboTNH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to take our movie, and extract all of the pixels frame-by-frame. We're going to store each pixel in a new <code>numpy</code> array."
      ],
      "metadata": {
        "id": "OK5N9XcQTi91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty arrays to store information about video\n",
        "coordinates_list_LED = []\n",
        "activity_matrix_LED = np.zeros((LED_video_y_pixels * LED_video_x_pixels, LED_video_frames))\n",
        "pixel_coordinates_LED = np.zeros((LED_video_y_pixels * LED_video_x_pixels, 2)) #two columns for x and y coords\n",
        "pixel_counter_LED = 0\n",
        "\n",
        "startTime = time.time()\n",
        "# load data pixel-by-pixel into a numpy array\n",
        "for i in range(LED_video_y_pixels):\n",
        "    for j in range(LED_video_x_pixels):\n",
        "        activity = np.array(LED_video[:, i, j])\n",
        "\n",
        "        activity_matrix_LED[pixel_counter_LED] = normalizeData(activity)\n",
        "        pixel_coordinates_LED[pixel_counter_LED] = np.array([j,i])\n",
        "        coordinates_list_LED.append((j,i))\n",
        "\n",
        "        pixel_counter_LED += 1\n",
        "\n",
        "print(\"Run time: {0} seconds\".format(time.time() - startTime))"
      ],
      "metadata": {
        "id": "pX8AyATnUApD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now convert our <code>numpy</code> array into a <code>pandas</code> dataframe:"
      ],
      "metadata": {
        "id": "wC7OluZJUITM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = range(1, activity_matrix_LED.shape[1]+1) # setting column values to be frame numbers\n",
        "columns = [\"{:02d}\".format(x) for x in columns]\n",
        "\n",
        "# convert numpy to panda data frame:\n",
        "df = pd.DataFrame(data=activity_matrix_LED, columns=columns)\n",
        "# last column represents coordinates:\n",
        "df['coords'] = coordinates_list_LED\n",
        "\n",
        "# show top 5 rows:\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Xu3Uwes6UFoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Find the <code>shape</code> of your new dataframe (<code>df</code>). Why does the number of rows make sense? For reference, look 3 code cells above where you calculated the shape of the raw video."
      ],
      "metadata": {
        "id": "lQF2snf7UjVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Figure out the shape of your dataframe"
      ],
      "metadata": {
        "id": "Y8QCyTXIV_Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Create a variable called <code>time</code>. The sampling rate of the camera used to image these data was 5 fps. Use this information to make the last column of your <code>time</code> variable equal to the length of the imaging session, in seconds, with a length equal to the number of frames in the video."
      ],
      "metadata": {
        "id": "_BqQsatBU-ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make your time variable here!\n"
      ],
      "metadata": {
        "id": "GCG5cn6OV93r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Showing Spatial Information for Each Pixel\n",
        "\n",
        "Let's extract the traces from four example ROIs, and show where those ROIs are located in our original imaging movie:"
      ],
      "metadata": {
        "id": "psut1zFxWJ9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pick sample pixels by coordinates, assign color to each\n",
        "example_rois = {(60, 100):'blue', (30,60):'orange', (83, 110):'green', (120,80):'red'}\n",
        "\n",
        "# define size and background color of the figure\n",
        "fig = plt.figure(constrained_layout=False, figsize=(18,6))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "# create grid layout with 4 rows and 2 columns\n",
        "gs = fig.add_gridspec(4, 2, hspace=0, wspace= 0)\n",
        "\n",
        "# first subplot will occupy 4 rows in the 1st column\n",
        "ax1 = fig.add_subplot(gs[0:4, 0])\n",
        "ax1.text(-10, -10, 'A', size=18)\n",
        "\n",
        "# turn off axis and labels\n",
        "ax1.axes.yaxis.set_visible(False)\n",
        "ax1.axes.xaxis.set_visible(False)\n",
        "ax1.imshow(video_first_frame, cmap='gray') #showing the first frame\n",
        "\n",
        "for roi in range(0, len(example_rois)):\n",
        "\n",
        "    ax = fig.add_subplot(gs[roi, 1])\n",
        "\n",
        "    key = list(example_rois.keys())[roi]\n",
        "    pixel_activity = np.array(df.loc[df['coords'] == (key[0], key[1])])[0][:700] #collecting activity from the dataframe\n",
        "    ax.plot(time, pixel_activity, color=example_rois[key])\n",
        "    ax.axis('off')\n",
        "    # add roi location to the image\n",
        "    ax1.scatter(key[0],key[1],s=110, facecolor=example_rois[key], edgecolor='white', linewidths=4)\n",
        "    if roi==0:\n",
        "        ax.text(-17, 1.3, 'B', size=18)\n",
        "\n",
        "\n",
        "ax.text(-10, 1.6, 'ROI activity', size=18,rotation='vertical')\n",
        "\n",
        "# turn axes back on for the last line plot\n",
        "ax.axis('on')\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['bottom'].set_visible(False)\n",
        "ax.spines['left'].set_visible(False)\n",
        "ax.axes.yaxis.set_visible(False)\n",
        "ax.set_xlabel('seconds')"
      ],
      "metadata": {
        "id": "PNe70hedWugK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fitting the LED Model to the Data\n",
        "\n",
        "We would now like to see which pixels generate a fluorescent signal that mimics the LED. If we find such pixels, we could conclude that the areas of the brain where the pixels are respond to light input. To do this, we'll first make our LED model:"
      ],
      "metadata": {
        "id": "yUjS6ywSlu5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(constrained_layout=True, figsize=(6,2));\n",
        "plt.plot(time, modelOnset(), color='red', label='onset model')\n",
        "plt.title('LED onset model');\n",
        "plt.xlabel('seconds');"
      ],
      "metadata": {
        "id": "S_M-zTnLmFmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And then try fitting it to our data by correlating each pixel's Î”F/F trace with the LED signal:"
      ],
      "metadata": {
        "id": "yfGoLgx4mL5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_onset = [] # saving coordinates and correlation coefficient of pixels\n",
        "correlation_map = np.zeros((LED_video_y_pixels, LED_video_x_pixels)) # image that holds correlation values per pixel\n",
        "\n",
        "for i in range(len(df)):\n",
        "    activity_correlation = np.array(df.iloc[i])[:700] # pixel activity\n",
        "    activity_coordinates = df.iloc[i]['coords']       # pixel coordinates\n",
        "\n",
        "    p_onset = pearsonr(activity_correlation, modelOnset())[0] #grabbing r corr value\n",
        "\n",
        "    # only save pixels that have correlation with model > 0.7\n",
        "    if (p_onset**2) > .7:\n",
        "        correlation_onset.append([activity_coordinates, p_onset**2]) #adding activity of correlated pixels\n",
        "\n",
        "    correlation_map[activity_coordinates[1], activity_coordinates[0]] = p_onset ** 2 #y direction first, x direction when indexing video\n",
        "\n",
        "    correlation_LED_indices = [] #indices of correlated pixes in main dataframe\n",
        "for i in range(len(correlation_onset)):\n",
        "    correlation_LED_indices.append(df[df['coords'] == correlation_onset[i][0]].index.tolist()[0]) #gives back array so indexing 0\n",
        "\n",
        "correlation_onset_df = df.iloc[correlation_LED_indices] #creating data subset\n",
        "correlation_onset_df.head()\n"
      ],
      "metadata": {
        "id": "q255kRVImNhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've pulled out the pixels that are highly correlated with our LED stimulus, we can plot them as a function of space on the raw video:"
      ],
      "metadata": {
        "id": "ICmDLun-nDP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1234) # fix seed so that result is the same each time\n",
        "example_correlated_pixels = np.random.choice(range(0, len(correlation_onset_df)),size=6, replace=False)\n",
        "\n",
        "fig = plt.figure(constrained_layout=True, figsize=(12,11))\n",
        "fig.patch.set_facecolor('white')\n",
        "\n",
        "nrow = 8\n",
        "ncol = 2\n",
        "\n",
        "gs = fig.add_gridspec(nrow, ncol, width_ratios=[1, 1])\n",
        "\n",
        "ax1 = fig.add_subplot(gs[0:4, 0])\n",
        "ax1.set_title('Map of correlation to LED onset')\n",
        "ax1.axes.yaxis.set_visible(False)\n",
        "ax1.axes.xaxis.set_visible(False)\n",
        "\n",
        "# add \"subfigure\" label\n",
        "ax1.text(-30, -25, 'A', size=18)\n",
        "plt1 = ax1.imshow(correlation_map, cmap='viridis', vmin=0.0, vmax=1.0) #displaying the correlation map\n",
        "fig.colorbar(plt1, ax=ax1,fraction=0.036, pad=0.04)\n",
        "\n",
        "\n",
        "\n",
        "ax2 = fig.add_subplot(gs[4:8, 0])\n",
        "ax2title = 'Location of {0} correlated pixels'.format(len(example_correlated_pixels))\n",
        "ax2.set_title(ax2title)\n",
        "ax2.axes.yaxis.set_visible(False)\n",
        "ax2.axes.xaxis.set_visible(False)\n",
        "plt2 = ax2.imshow(correlation_map, cmap='gray', vmin=0.0, vmax=1.0)\n",
        "#fig.colorbar(plt2, ax=ax2)\n",
        "\n",
        "\n",
        "for pixel_ind in example_correlated_pixels: #displaying the spatial location of example correlated pixels\n",
        "    ax2.scatter(correlation_onset_df.iloc[pixel_ind]['coords'][0],\n",
        "                correlation_onset_df.iloc[pixel_ind]['coords'][1],\n",
        "                s=50, facecolor='none', edgecolor='#6699ff', linewidths=2)\n",
        "\n",
        "ax3 = fig.add_subplot(gs[0, 1])\n",
        "ax3.plot(modelOnset(), color='red', label='onset model') #plotting the LED onset model\n",
        "ax3.set_title('LED onset model', loc='left')\n",
        "ax3.axis('off')\n",
        "ax3.text(-70, 0.4, 'B', size=18)\n",
        "\n",
        "# for each ROI out of 6 we will plot line profile:\n",
        "for roi in range(0,6):\n",
        "    ax_roi = fig.add_subplot(gs[1+roi, 1])\n",
        "    x_roi = correlation_onset_df.iloc[example_correlated_pixels[roi]]['coords'][0]\n",
        "    y_roi = correlation_onset_df.iloc[example_correlated_pixels[roi]]['coords'][1]\n",
        "    pixel_activity = np.array(LED_video[:, y_roi, x_roi]) #collecting activity from the pixel in LED video\n",
        "    ax_roi.plot(pixel_activity, color='#6699ff')\n",
        "    ax_roi.axis('off')\n",
        "\n",
        "\n",
        "# overlay all traces with correlation > 0.7 to show spread:\n",
        "ax_overlay = fig.add_subplot(gs[7,1])\n",
        "for i in range(len(correlation_onset_df)):\n",
        "    ax_overlay.plot(xaxis, correlation_onset_df.iloc[i][:700], color='gray', alpha=.05)\n",
        "\n",
        "# calculate average activity of pixels correlated with model\n",
        "average_activity = np.array(correlation_onset_df.drop('coords', axis=1).mean(axis=0))\n",
        "ax_overlay.plot(xaxis, average_activity, color='black')\n",
        "ax_overlay.spines['top'].set_visible(False)\n",
        "ax_overlay.spines['right'].set_visible(False)\n",
        "ax_overlay.spines['bottom'].set_visible(False)\n",
        "ax_overlay.spines['left'].set_visible(False)\n",
        "ax_overlay.axes.yaxis.set_visible(False)\n",
        "ax_overlay_title = '{0} pixels correlated with model'.format(len(correlation_onset_df))\n",
        "ax_overlay.set_title(ax_overlay_title, loc='left')\n",
        "ax_overlay.set_xlabel('seconds')\n"
      ],
      "metadata": {
        "id": "coXvdmDFnSVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Is there a relationship between pixels that respond to the LED input and location in the zebrafish pallium?"
      ],
      "metadata": {
        "id": "R24zAODjndUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mouse Fear Conditioning Data\n",
        "\n",
        "We're going to switch gears a bit now and look at 1-photon miniscope imaging data from the mouse ACC during fear conditioning - specifically, when the mouse is learning to associate a brief (2 s) foot-shock with a tone."
      ],
      "metadata": {
        "id": "DRMs913znko8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = 'https://drive.google.com/uc?id=1xdzq1ctctB9cY0S5VdiJtOErDKUcVaNS'>"
      ],
      "metadata": {
        "id": "hJVW0c_CGg5k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a miniature 1-photon miniscope to image GFP signals from ACC neurons during the entire conditioning session. We are interested to know if these signals correspond to events during the session - specifically, the tone, and the shock."
      ],
      "metadata": {
        "id": "dUzfGxK7Gr8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the extracted ROI traces from our GitHub repo. To do this, import the <code>fear_traces.csv</code> file as a <code>numpy</code> array called <code>fear_traces</code>."
      ],
      "metadata": {
        "id": "2QGrPdYSHGEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your fluorescent traces!"
      ],
      "metadata": {
        "id": "j1dknpdOHSeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Look at the <code>shape</code> of your <code>fear_traces</code> variable. Columns correspond to individual cells, and rows correspond to samples. The sampling miniscope's sampling rate is 15 fps. How many cells do we have in our array? What is the length of the conditioning session?"
      ],
      "metadata": {
        "id": "lCeOYbDNHU5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here!"
      ],
      "metadata": {
        "id": "oA08bJmNHnVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Create a <code>time</code> variable for your imaging data. Plot fluorescent traces from the first two cells in your <code>fear_traces</code> array in one plot, with time on the x-axis. Label your axes."
      ],
      "metadata": {
        "id": "t3uRFOYZH_2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here!\n"
      ],
      "metadata": {
        "id": "OeB9VWorIXhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question: Do you notice any time periods where the fluorescent intensity tends to go up or down?"
      ],
      "metadata": {
        "id": "4Gh8efPhIZRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Next, we need some way of knowing when tones and shocks occurred. The <code>shock_index.csv</code> and <code>tone_index.csv</code> files on our GitHub repo provide us with timesetamps for each tone and each shock during the conditioning session. Import those files as <code>numpy</code> arrays called <code>shock_index</code> and <code>tone_index</code>, respectively."
      ],
      "metadata": {
        "id": "NlwKA7gpIm1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import your indices\n",
        "\n",
        "shock_index = np.loadtxt('https://raw.githubusercontent.com/hallockh/neur_265/main/shock_index.csv', delimiter = ',').astype(int)\n",
        "tone_index = np.loadtxt('https://raw.githubusercontent.com/hallockh/neur_265/main/tone_index.csv', delimiter = ',').astype(int)"
      ],
      "metadata": {
        "id": "z1I5VhckI5Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "><b>Task:</b> Make a plot with the fluorescent trace from the first cell, and black horizontal lines at times that the mouse received a foot-shock. Put time on the x-axis. Do you notice any changes in fluorescent activity during these timepoints?"
      ],
      "metadata": {
        "id": "ID5p7Rm36yof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make your plot here!\n"
      ],
      "metadata": {
        "id": "p9Nr_3fl7LWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way of visualizing this would be to isolate snippets of the fluorescent trace from each cell surrounding shock onset and tone onset, and looking at them altogether in a heatmap."
      ],
      "metadata": {
        "id": "jWn34yeo7oem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's isolate our shock-centered and tone-centered data\n",
        "\n",
        "# Initialize variables\n",
        "shock_temp = np.zeros((45, 3))\n",
        "shock_traces = np.zeros((45, 10))\n",
        "tone_temp = np.zeros((45, 3))\n",
        "tone_traces = np.zeros((45, 10))\n",
        "\n",
        "# Make a meaningful x-axis for our plots\n",
        "xaxis = np.linspace(-1,2,45)\n",
        "\n",
        "# Grab fluorescence surrounding shock onset for each ROI\n",
        "for i in range(10):\n",
        "  for j in range(3):\n",
        "    shock_temp[:,j] = fear_traces[(shock_index[j]-1)*15:(shock_index[j]+2)*15,i]\n",
        "  shock_traces[:,i] = np.mean(shock_temp, axis = 1)\n",
        "\n",
        "# Grab fluorescence surrounding tone onset for each ROI\n",
        "for i in range(10):\n",
        "  for j in range(3):\n",
        "    tone_temp[:,j] = fear_traces[(tone_index[j]-1)*15:(tone_index[j]+2)*15,i]\n",
        "  tone_traces[:,i] = np.mean(tone_temp, axis = 1)\n",
        "\n",
        "f, a = plt.subplots(1,2,figsize=(6, 6))\n",
        "\n",
        "a[0].imshow(np.transpose(shock_traces), cmap = 'coolwarm', extent=[xaxis[0], xaxis[-1], 1, 10], aspect = 'auto')\n",
        "a[0].plot([0, 0], [1,10], 'k', lw=2) # Make a black line at shock onset\n",
        "a[0].set_xlabel('Time from Shock Onset [s]')\n",
        "a[0].set_ylabel('ROI')\n",
        "\n",
        "a[1].imshow(np.transpose(tone_traces), cmap = 'coolwarm', extent=[xaxis[0], xaxis[-1], 1, 10], aspect = 'auto')\n",
        "a[1].plot([0, 0], [1,10], 'k', lw=2) # Make a black line at tone onset\n",
        "a[1].set_xlabel('Time from Tone Onset [s]')\n",
        "\n"
      ],
      "metadata": {
        "id": "RK0OLOhe72k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like cells in the ACC have a consistent relationship with shock onset, and a less consistent relationship with tone onset."
      ],
      "metadata": {
        "id": "WE6fqCbmL5ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling our Fear Data\n",
        "\n",
        "To see whether tone presentation, shock presentation, or a combination of the two affect cellular activity in the ACC during conditioning, we can use generalized linear modeling (GLM) with tone onset and shock onset as covariates. We've already learned that spiking tends to approximate a Poisson (random) distribution. Since calcium-induced fluorescence is closely tied to spiking (though not equal to spiking!), we can also assume that our fluorescent signal approximates a Poisson distribution. When looking at our first fluorescent trace, we notice that fluorescence seems to increase exponentially shortly following shock onset."
      ],
      "metadata": {
        "id": "We_tAylx7V2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing we need to do is make our <code>predictor</code> (covariates in the model) arrays. To do that, we'll need to make one array for shock onset, and one array for tone onset. We'll make both arrays equal to the length of our fluorescent trace data (10458 samples). Since shock events and tone events are binary (a tone/shock is either happening, or it's not), we'll populate our arrays with \"zeros\" when tones/shocks aren't happening, and \"ones\" when they are."
      ],
      "metadata": {
        "id": "2u8CR1u9M3ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shock_array = np.zeros(10458)\n",
        "for i in range(10458):\n",
        "  if i >= (shock_index[0]-15)*15 and i <= (shock_index[0]+30)*15:\n",
        "    shock_array[i] = 1\n",
        "  elif i>= (shock_index[1]-15)*15 and i <= (shock_index[1]+30)*15:\n",
        "    shock_array[i] = 1\n",
        "  elif i>= (shock_index[2]-15)*15 and i <= (shock_index[2]+30)*15:\n",
        "    shock_array[i] = 1\n",
        "  else:\n",
        "    shock_array[i] = 0\n",
        "\n",
        "tone_array = np.zeros(10458)\n",
        "for i in range(10458):\n",
        "  if i >= (tone_index[0]-15)*15 and i <= (tone_index[0]+30)*15:\n",
        "    tone_array[i] = 1\n",
        "  elif i>= (tone_index[1]-15)*15 and i <= (tone_index[1]+30)*15:\n",
        "    tone_array[i] = 1\n",
        "  elif i>= (tone_index[2]-15)*15 and i <= (tone_index[2]+30)*15:\n",
        "    tone_array[i] = 1\n",
        "  else:\n",
        "    tone_array[i] = 0"
      ],
      "metadata": {
        "id": "XR5Qo2uWwDHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first make a model that uses only tone onset as a predictor, and see how it does with the fluorescent trace from the first ROI:"
      ],
      "metadata": {
        "id": "_vZ1ynKZN6G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import DataFrame as df\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.genmod.families import Poisson\n",
        "from statsmodels.genmod.families.links import identity, log\n",
        "from scipy.stats import chi2\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "first_trace = fear_traces[:,0]\n",
        "\n",
        "predictors = df(data={'Intercept': np.ones_like(tone_array), 'tone onset': tone_array})\n",
        "model1 = sm.GLM(first_trace, predictors, family=Poisson())\n",
        "model1_results = model1.fit() # Fit model to our data\n",
        "b1 = model1_results.params\n",
        "print('b1:')\n",
        "print(b1)"
      ],
      "metadata": {
        "id": "malsSFND1TiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how our model did, we can plot the model's predicted fluorescent trace on top of the actual trace from our first ROI:"
      ],
      "metadata": {
        "id": "1ZwjseThOdkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plottin' tone onset model results\n",
        "\n",
        "plt.plot(time, model1_results.predict(), label = \"predicted trace\")\n",
        "plt.plot(time, first_trace, label = \"actual trace\")\n",
        "plt.xlabel('Time [s]')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "6nNIBikIOjkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not so great! This might have been predictable for a few reasons:\n",
        "\n",
        "1) We saw from our visualizations above that there seemed to be an inconsistent relationship between tone onset and calcium fluorescence in our population, and\n",
        "\n",
        "2) Tone onset only accounts for three timepoints during the imaging session.\n",
        "\n",
        "Let's see what happens when we make a second model with shock onset as the predictor:"
      ],
      "metadata": {
        "id": "R-6v1v4MQBjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictors2 = df(data={'Intercept': np.ones_like(shock_array), 'shock onset': shock_array})\n",
        "model2 = sm.GLM(first_trace, predictors2, family=Poisson())\n",
        "model2_results = model2.fit() # Fit model to our data\n",
        "b2 = model2_results.params    # Get the predicted coefficient vector\n",
        "print('b2:\\n', b2)"
      ],
      "metadata": {
        "id": "4enB6m65QjAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the relationship between predicted fluorescence and actual fluorescence for this model:"
      ],
      "metadata": {
        "id": "jatxXxLjQylC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plottin' shock onset model results\n",
        "\n",
        "plt.plot(time, model2_results.predict(), label = \"predicted trace\")\n",
        "plt.plot(time, first_trace, label = \"actual trace\")\n",
        "plt.xlabel('Time [s]')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "PjQKA3CjQ2bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like this neuron isn't just responding to shock onset or tone onset. Maybe it responds to a combinaton of the two? To test this, we'll add both into our <code>predictors</code> variable, and re-run our model:"
      ],
      "metadata": {
        "id": "xCiNX-bKRU_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictors['shock onset'] = shock_array\n",
        "model3 = sm.GLM(first_trace, predictors, family=Poisson())\n",
        "model3_results = model3.fit() # Fit model to our data\n",
        "b1 = model3_results.params\n",
        "print('b1:')\n",
        "print(b1)\n"
      ],
      "metadata": {
        "id": "ppknTTpT3zDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And plot expected vs. actual traces:"
      ],
      "metadata": {
        "id": "8ZDMYHKfUp98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plottin' tone and shock onset model results\n",
        "\n",
        "plt.plot(time, model3_results.predict(), label = \"predicted trace\")\n",
        "plt.plot(time, first_trace, label = \"actual trace\")\n",
        "plt.xlabel('Time [s]')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "zt5zh6USRtXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that this model seems to capture a lot of the broad variation in fluorescence we see after the first tone. It makes our predicted signal is completely flat outside of these time periods - there aren't any covariates at these times, so the model has nothing to fit. There are likely some continuous variables we could add to the model - for example, the animal's motion, head direction, etc. - that would account for more of the variation in fluorescence outside of the tone/shock periods."
      ],
      "metadata": {
        "id": "oapw1318RqZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we can test goodness-of-fit between nested models with chi-square tests. Let's see how our third model holds up against the first two models:"
      ],
      "metadata": {
        "id": "Qj_B5c3cVfv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev1 = model1_results.deviance\n",
        "dev2 = model2_results.deviance\n",
        "dev3 = model3_results.deviance\n",
        "p1 = 1 - chi2.cdf(dev1 - dev3, 1)  # Compare Models 1 and 3, nested GLMs.\n",
        "p2 = 1 - chi2.cdf(dev2 - dev3, 1)  # Compare Models 2 and 3, nested GLMs.\n",
        "print(p1)\n",
        "print(p2)"
      ],
      "metadata": {
        "id": "N_gFBS68R_aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another powerful way of analyzing how well a model performs for an entire population of neurons is to calculate the correlation between the model's predicted trace and the actual trace for each neuron, and plot the resulting *r* values as a cumulative distribution for each model:"
      ],
      "metadata": {
        "id": "zwxjPDY8VyTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cumulative distribution of r values for model 1 (just tone onset)\n",
        "\n",
        "predictors = df(data={'Intercept': np.ones_like(tone_array), 'tone onset': tone_array})\n",
        "r_model1 = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "  trace_temp = fear_traces[:,i]\n",
        "  model1 = sm.GLM(trace_temp, predictors, family=Poisson())\n",
        "  model1_results = model1.fit()\n",
        "  predicted_trace = model1_results.predict()\n",
        "  r1 = pearsonr(trace_temp, predicted_trace)\n",
        "  r_model1[i] = r1.statistic\n",
        "\n",
        "# For model 2\n",
        "\n",
        "predictors2 = df(data={'Intercept': np.ones_like(shock_array), 'shock onset': shock_array})\n",
        "r_model2 = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "  trace_temp = fear_traces[:,i]\n",
        "  model2 = sm.GLM(trace_temp, predictors2, family=Poisson())\n",
        "  model2_results = model2.fit()\n",
        "  predicted_trace = model2_results.predict()\n",
        "  r2 = pearsonr(trace_temp, predicted_trace)\n",
        "  r_model2[i] = r2.statistic\n",
        "\n",
        "# For model 3\n",
        "\n",
        "predictors['shock onset'] = shock_array\n",
        "r_model3 = np.zeros(10)\n",
        "\n",
        "for i in range(10):\n",
        "  trace_temp = fear_traces[:,i]\n",
        "  model3 = sm.GLM(trace_temp, predictors, family=Poisson())\n",
        "  model3_results = model3.fit()\n",
        "  predicted_trace = model3_results.predict()\n",
        "  r3 = pearsonr(trace_temp, predicted_trace)\n",
        "  r_model3[i] = r3.statistic\n",
        "\n",
        "plt.plot(np.sort(r_model1), np.linspace(0, 1, len(r_model1), endpoint=False), label = \"model 1\")\n",
        "plt.plot(np.sort(r_model2), np.linspace(0, 1, len(r_model2), endpoint=False), label = \"model 2\")\n",
        "plt.plot(np.sort(r_model3), np.linspace(0, 1, len(r_model3), endpoint=False), label = \"model 3\")\n",
        "plt.legend()\n",
        "plt.xlabel('Correlation Between Model Prediction and Observed Trace')\n",
        "plt.ylabel('Cumulative Proportion of ROIs')"
      ],
      "metadata": {
        "id": "dBU1__sXV-pA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}